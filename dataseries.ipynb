{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Time series forecasting (optional)\n",
        "from prophet import Prophet\n",
        "\n",
        "# Clustering (optional)\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# For warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "i-14vR5QpiIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load and Merge the Datasets\n",
        "# Load the dispensing and weather datasets\n",
        "df_dispense = pd.read_csv('Dispensing_travad.csv')\n",
        "df_weather = pd.read_csv('final_cleaned_data_sand_ocean_weather (1).csv')\n"
      ],
      "metadata": {
        "id": "DeoFIBdRpkGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dispense.head()"
      ],
      "metadata": {
        "id": "ie4pNfZrMuUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the date columns to datetime objects\n",
        "df_dispense['Date'] = pd.to_datetime(df_dispense['Date'], errors='coerce')\n",
        "df_weather['utc_date'] = pd.to_datetime(df_weather['utc_date'], errors='coerce')"
      ],
      "metadata": {
        "id": "jWJn4QGKyYFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string column for merging in a common format (YYYY-MM-DD)\n",
        "df_dispense['date_str'] = df_dispense['Date'].dt.strftime('%Y-%m-%d')\n",
        "df_weather['date_str'] = df_weather['utc_date'].dt.strftime('%Y-%m-%d')"
      ],
      "metadata": {
        "id": "pCIW-Xysydeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the datasets on 'date_str'\n",
        "merged_df = pd.merge(df_dispense, df_weather, on='date_str', how='inner')\n",
        "print(\"Merged DataFrame shape:\", merged_df.shape)\n",
        "\n",
        "# Inspect a few rows\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "YH8wnMvFygtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Data Cleaning & Feature Engineering\n",
        "# Convert key columns to numeric\n",
        "merged_df['Volume'] = pd.to_numeric(merged_df['Volume'], errors='coerce')\n",
        "merged_df['air_temp_c'] = pd.to_numeric(merged_df['air_temp_c'], errors='coerce')"
      ],
      "metadata": {
        "id": "iilreimay4G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize Volume and Air Temperature\n",
        "scaler = StandardScaler()\n",
        "merged_df['Volume_std'] = scaler.fit_transform(merged_df[['Volume']])\n",
        "merged_df['air_temp_c_std'] = scaler.fit_transform(merged_df[['air_temp_c']])"
      ],
      "metadata": {
        "id": "ntIv8SEsy71A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When computing the ratio, we use the standardized values.\n",
        "# To avoid division by zero, we filter out rows where air_temp_c_std is 0.\n",
        "merged_df = merged_df[merged_df['air_temp_c_std'] != 0]"
      ],
      "metadata": {
        "id": "L6ntqWj_zJc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the volume-to-temp ratio (in percentage) using standardized values.\n",
        "merged_df['volume_to_temp_ratio'] = (merged_df['Volume_std'] / merged_df['air_temp_c_std']) * 100"
      ],
      "metadata": {
        "id": "HHizrq7czM1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "GXxtTclAQDzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show summary statistics\n",
        "print(\"Volume-to-Temp Ratio Statistics:\")\n",
        "print(merged_df['volume_to_temp_ratio'].describe())"
      ],
      "metadata": {
        "id": "CMNlwuzfzP8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Create Three-Class Ratio Labels for Classification\n",
        "# Calculate the 33rd and 66th percentiles for the volume_to_temp_ratio\n",
        "low_threshold = merged_df['volume_to_temp_ratio'].quantile(0.33)\n",
        "high_threshold = merged_df['volume_to_temp_ratio'].quantile(0.66)\n",
        "\n",
        "def assign_three_classes(x):\n",
        "    if x <= low_threshold:\n",
        "        return 'Low'\n",
        "    elif x <= high_threshold:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'High'\n",
        "\n",
        "# Create the three-class column\n",
        "merged_df['ratio_class_3'] = merged_df['volume_to_temp_ratio'].apply(assign_three_classes)\n",
        "\n",
        "# Check the distribution of the three classes\n",
        "print(\"Three-Class Ratio Distribution:\")\n",
        "print(merged_df['ratio_class_3'].value_counts())\n"
      ],
      "metadata": {
        "id": "NUvYi6WGzWNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "01QZd-lE0oHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download merged_df.nunique()\n",
        "\n",
        "merged_df.nunique().to_csv('nunique_values.csv')\n"
      ],
      "metadata": {
        "id": "qh8LnXirTYUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.nunique()"
      ],
      "metadata": {
        "id": "JcIWXTe3S96Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame for rows classified as 'Medium'\n",
        "low_df = merged_df[merged_df['ratio_class_3'] == 'Low']\n",
        "\n",
        "# Display summary statistics for the Medium class\n",
        "print(\"Summary statistics for Low class:\")\n",
        "print(low_df['volume_to_temp_ratio'].describe())\n",
        "\n",
        "# Visualize the distribution of volume_to_temp_ratio for the Medium class\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(low_df['volume_to_temp_ratio'], bins=10, kde=True, color='orange')\n",
        "plt.title('Volume-to-Temp Ratio Distribution for Low Class')\n",
        "plt.xlabel('Volume-to-Temp Ratio (%)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IM20EEx_-Meb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame for rows classified as 'Medium'\n",
        "medium_df = merged_df[merged_df['ratio_class_3'] == 'Medium']\n",
        "\n",
        "# Display summary statistics for the Medium class\n",
        "print(\"Summary statistics for Medium class:\")\n",
        "print(medium_df['volume_to_temp_ratio'].describe())\n",
        "\n",
        "# Visualize the distribution of volume_to_temp_ratio for the Medium class\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(medium_df['volume_to_temp_ratio'], bins=30, kde=True, color='orange')\n",
        "plt.title('Volume-to-Temp Ratio Distribution for Medium Class')\n",
        "plt.xlabel('Volume-to-Temp Ratio (%)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ELPfOhRL40Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame for rows classified as 'High'\n",
        "low_df = merged_df[merged_df['ratio_class_3'] == 'High']\n",
        "\n",
        "# Display summary statistics for the High class\n",
        "print(\"Summary statistics for High class:\")\n",
        "print(low_df['volume_to_temp_ratio'].describe())\n",
        "\n",
        "# Visualize the distribution of volume_to_temp_ratio for the High class\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(low_df['volume_to_temp_ratio'], bins=10, kde=True, color='orange')\n",
        "plt.title('Volume-to-Temp Ratio Distribution for High Class')\n",
        "plt.xlabel('Volume-to-Temp Ratio (%)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZRZtJwBl_Z-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of values in each class (Low, Medium, High)\n",
        "class_counts = merged_df['ratio_class_3'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Count of values for each ratio class:\")\n",
        "print(class_counts)\n"
      ],
      "metadata": {
        "id": "OWvfmvi18ZeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the counts for each ratio class\n",
        "class_counts = merged_df['ratio_class_3'].value_counts()\n",
        "\n",
        "# Reorder the counts to ensure they appear as Low, Medium, High\n",
        "ordered_classes = ['Low', 'Medium', 'High']\n",
        "ordered_counts = class_counts.reindex(ordered_classes).fillna(0)\n",
        "\n",
        "# Create the bar graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x=ordered_counts.index, y=ordered_counts.values, palette='Set2')\n",
        "\n",
        "# Set title and axis labels\n",
        "plt.title(\"Number of Observations by Ratio Class\")\n",
        "plt.xlabel(\"Ratio Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# Annotate each bar with its count\n",
        "for i, count in enumerate(ordered_counts.values):\n",
        "    plt.text(i, count + count*0.01, f\"{int(count)}\", ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y0jyuPDbRKTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the minimum and maximum values for each class\n",
        "low_stats = merged_df[merged_df['ratio_class_3'] == 'Low']['volume_to_temp_ratio'].agg(['min', 'max'])\n",
        "med_stats = merged_df[merged_df['ratio_class_3'] == 'Medium']['volume_to_temp_ratio'].agg(['min', 'max'])\n",
        "high_stats = merged_df[merged_df['ratio_class_3'] == 'High']['volume_to_temp_ratio'].agg(['min', 'max'])\n",
        "\n",
        "# Define categories and collect their min and max values\n",
        "categories = ['Low', 'Medium', 'High']\n",
        "mins = [low_stats['min'], med_stats['min'], high_stats['min']]\n",
        "maxs = [low_stats['max'], med_stats['max'], high_stats['max']]\n",
        "\n",
        "# Create a horizontal range graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot a horizontal line for each category representing its range\n",
        "for i, cat in enumerate(categories):\n",
        "    plt.hlines(y=i, xmin=mins[i], xmax=maxs[i], color='blue', linewidth=5)\n",
        "    # Plot the minimum and maximum as markers\n",
        "    plt.plot(mins[i], i, 'go', markersize=10)  # Green marker for min\n",
        "    plt.plot(maxs[i], i, 'ro', markersize=10)  # Red marker for max\n",
        "    # Annotate the endpoints with their exact values\n",
        "    plt.text(mins[i], i - 0.1, f\"Min: {mins[i]:.2f}\", color='green', fontsize=10, ha='left')\n",
        "    plt.text(maxs[i], i - 0.1, f\"Max: {maxs[i]:.2f}\", color='red', fontsize=10, ha='right')\n",
        "\n",
        "# Set y-axis labels and title\n",
        "plt.yticks(range(len(categories)), categories)\n",
        "plt.xlabel(\"Volume-to-Temp Ratio (%)\")\n",
        "plt.title(\"Range of Volume-to-Temp Ratio by Classification\")\n",
        "plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oo6LKE9IcOgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.columns"
      ],
      "metadata": {
        "id": "fvZ3h7VvqZ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = merged_df[['air_temp_c', 'humidity_percent', 'dewpoint_temp_c', 'precipitation_mm', 'volume_to_temp_ratio', 'wind_direction_deg']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Key Features and Target')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FriuGw_2dvvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Classification Task: Three-Class Ratio Prediction\n",
        "# ================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Assuming merged_df is already defined and contains the relevant columns.\n",
        "# Prepare the features and map the class labels to numeric values.\n",
        "features_class = ['air_temp_c_std', 'humidity_percent', 'dewpoint_temp_c']\n",
        "merged_df['ratio_class_3_numeric'] = merged_df['ratio_class_3'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
        "\n",
        "X_class = merged_df[features_class]\n",
        "y_class = merged_df['ratio_class_3_numeric']\n",
        "\n",
        "print(\"Classification Features:\", features_class)\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize the classification features\n",
        "scaler_class = StandardScaler()\n",
        "X_train_class_scaled = scaler_class.fit_transform(X_train_class)\n",
        "X_test_class_scaled = scaler_class.transform(X_test_class)\n",
        "\n",
        "# ----- Model 1: Logistic Regression -----\n",
        "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train_class_scaled, y_train_class)\n",
        "y_pred_log = log_reg.predict(X_test_class_scaled)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test_class, y_pred_log))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_class, y_pred_log))\n",
        "\n",
        "# ----- Model 2: Random Forest Classifier -----\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train_class_scaled, y_train_class)\n",
        "y_pred_rf = rf_clf.predict(X_test_class_scaled)\n",
        "print(\"\\nRandom Forest Classifier Classification Report:\")\n",
        "print(classification_report(y_test_class, y_pred_rf))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_class, y_pred_rf))\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# ----- Alternative Model 3: Decision Tree Classifier -----\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train_class_scaled, y_train_class)\n",
        "y_pred_dt = dt_clf.predict(X_test_class_scaled)\n",
        "print(\"\\nDecision Tree Classifier Classification Report:\")\n",
        "print(classification_report(y_test_class, y_pred_dt))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_class, y_pred_dt))\n"
      ],
      "metadata": {
        "id": "cmPnsTCBG5ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Performance Summary -----\n",
        "performance_metrics = {}\n",
        "\n",
        "# Logistic Regression Performance\n",
        "acc_log = accuracy_score(y_test_class, y_pred_log)\n",
        "report_log = classification_report(y_test_class, y_pred_log, output_dict=True)\n",
        "performance_metrics['Logistic Regression'] = {\n",
        "    'Accuracy': acc_log,\n",
        "    'Macro Precision': report_log['macro avg']['precision'],\n",
        "    'Macro Recall': report_log['macro avg']['recall'],\n",
        "    'Macro F1': report_log['macro avg']['f1-score']\n",
        "}\n",
        "\n",
        "# Random Forest Performance\n",
        "acc_rf = accuracy_score(y_test_class, y_pred_rf)\n",
        "report_rf = classification_report(y_test_class, y_pred_rf, output_dict=True)\n",
        "performance_metrics['Random Forest'] = {\n",
        "    'Accuracy': acc_rf,\n",
        "    'Macro Precision': report_rf['macro avg']['precision'],\n",
        "    'Macro Recall': report_rf['macro avg']['recall'],\n",
        "    'Macro F1': report_rf['macro avg']['f1-score']\n",
        "}\n",
        "\n",
        "# Decision Tree Performance\n",
        "acc_dt = accuracy_score(y_test_class, y_pred_dt)\n",
        "report_dt = classification_report(y_test_class, y_pred_dt, output_dict=True)\n",
        "performance_metrics['Decision Tree'] = {\n",
        "    'Accuracy': acc_dt,\n",
        "    'Macro Precision': report_dt['macro avg']['precision'],\n",
        "    'Macro Recall': report_dt['macro avg']['recall'],\n",
        "    'Macro F1': report_dt['macro avg']['f1-score']\n",
        "}\n",
        "\n",
        "performance_df = pd.DataFrame(performance_metrics).T\n",
        "print(\"\\nSummary of Classification Model Performance:\")\n",
        "print(performance_df)"
      ],
      "metadata": {
        "id": "3KWf4WIZIRg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 6. Prepare Data for Classification and Regression Separately\n",
        "# ================================\n",
        "\n",
        "# For classification, we want to predict the ratio class, so our features should be only the weather measurements.\n",
        "# We do not include the continuous volume_to_temp_ratio here.\n",
        "features_class = ['air_temp_c', 'humidity_percent', 'dewpoint_temp_c']  # Use raw weather features\n",
        "# Map the three-class labels to numeric: Low=0, Medium=1, High=2\n",
        "merged_df['ratio_class_3_numeric'] = merged_df['ratio_class_3'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
        "\n",
        "X_class = merged_df[features_class]\n",
        "y_class = merged_df['ratio_class_3_numeric']\n",
        "\n",
        "# For regression, we want to predict the continuous volume-to-temp ratio.\n",
        "# Our features should again be only the weather measurements (and not include the ratio class).\n",
        "features_reg = ['air_temp_c', 'humidity_percent', 'dewpoint_temp_c']  # Use raw weather features\n",
        "X_reg = merged_df[features_reg]\n",
        "y_reg = merged_df['volume_to_temp_ratio']\n",
        "\n",
        "# Split the classification data (80% train, 20% test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the regression data (80% train, 20% test)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Classification Features:\", features_class)\n",
        "print(\"Regression Features:\", features_reg)\n",
        "\n"
      ],
      "metadata": {
        "id": "B6hMz7uopuFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the classification features\n",
        "scaler_class = StandardScaler()\n",
        "X_train_class_scaled = scaler_class.fit_transform(X_train_class)\n",
        "X_test_class_scaled = scaler_class.transform(X_test_class)\n",
        "\n",
        "# Train a Logistic Regression classifier (using multinomial option)\n",
        "clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_class_scaled, y_train_class)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_class = clf.predict(X_test_class_scaled)\n",
        "\n",
        "# Evaluate the classification model\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_class, y_pred_class))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_class, y_pred_class))\n"
      ],
      "metadata": {
        "id": "7V4sa3e5tWs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 7. Standardize Features for Both Tasks\n",
        "# ================================\n",
        "# Standardize features for classification\n",
        "scaler_class = StandardScaler()\n",
        "X_train_class = scaler_class.fit_transform(X_train_class)\n",
        "X_test_class = scaler_class.transform(X_test_class)\n",
        "\n",
        "# Standardize features for regression\n",
        "scaler_reg = StandardScaler()\n",
        "X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
        "X_test_reg = scaler_reg.transform(X_test_reg)\n"
      ],
      "metadata": {
        "id": "BrnhQ5F9p1Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 8. Classification: Three-Class Ratio Prediction\n",
        "# ================================\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train a Logistic Regression model for multi-class classification\n",
        "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train_class, y_train_class)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_class = log_reg.predict(X_test_class)\n",
        "print(\"Classification Report for Three-Class Ratio:\")\n",
        "print(classification_report(y_test_class, y_pred_class))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_class, y_pred_class))\n"
      ],
      "metadata": {
        "id": "yRAH2OhWp3Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define a reduced hyperparameter grid for each model\n",
        "param_dist = {\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 150, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Gradient Booster': {\n",
        "        'n_estimators': [100, 150],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    },\n",
        "    'decision Tree': {\n",
        "        'max_depth': [5, 10, 20],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Booster': GradientBoostingRegressor(random_state=42),\n",
        "    'decision Tree': DecisionTreeRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV to each model\n",
        "best_models = {}\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Tuning {model_name} using RandomizedSearchCV...\")\n",
        "    randomized_search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_distributions=param_dist[model_name],\n",
        "        n_iter=20,  # Number of combinations to sample\n",
        "        cv=3,  # Reduced cross-validation folds\n",
        "        n_jobs=-1,  # Use all available cores\n",
        "        random_state=42,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        verbose=1\n",
        "    )\n",
        "    randomized_search.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "    # Save the best model\n",
        "    best_models[model_name] = randomized_search.best_estimator_\n",
        "\n",
        "# Evaluate tuned models\n",
        "regression_results = {}\n",
        "for model_name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test_reg)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred))\n",
        "    r2 = r2_score(y_test_reg, y_pred)\n",
        "    regression_results[model_name] = {'RMSE': rmse, 'R2': r2}\n",
        "    print(f\"Tuned {model_name}: RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
        "\n",
        "# Summary DataFrame\n",
        "results_df = pd.DataFrame(regression_results).T\n",
        "print(\"\\n Tuning Results Summary:\")\n",
        "print(results_df)\n",
        "\n",
        "# Bar plot of R² scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=results_df.reset_index(), x='index', y='R2', palette='coolwarm')\n",
        "plt.axhline(0, linestyle='--', color='gray')\n",
        "plt.title('Tuned R² Scores of Regression Models')\n",
        "plt.ylabel('R² Score')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kyIV1qV-vD5",
        "outputId": "5f94e281-41df-4a50-d29c-06ff95bf2aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Random Forest using RandomizedSearchCV...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define your features and target\n",
        "features_reg = ['air_temp_c', 'humidity_percent', 'dewpoint_temp_c']\n",
        "target_reg = 'Average Flow Rate'\n",
        "# Prepare the data\n",
        "X_reg = merged_df[features_reg]\n",
        "y_reg = merged_df[target_reg]\n",
        "\n",
        "# Remove rows with missing values in the target variable\n",
        "X_reg = X_reg[y_reg.notna()]\n",
        "y_reg = y_reg[y_reg.notna()]\n",
        "\n",
        "#  Train-test split\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_reg = scaler.fit_transform(X_train_reg)\n",
        "X_test_reg = scaler.transform(X_test_reg)\n",
        "\n",
        "#  Define regression models\n",
        "regression_models = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=150, max_depth=10, random_state=42),\n",
        "    'Gradient Booster': GradientBoostingRegressor(n_estimators=150, learning_rate=0.1, max_depth=4, random_state=42),\n",
        "    'decision Tree':DecisionTreeRegressor(max_depth=10, random_state=42)}\n",
        "\n",
        "\n",
        "#  Collect results\n",
        "regression_results = {}\n",
        "\n",
        "for name, model in regression_models.items():\n",
        "    model.fit(X_train_reg, y_train_reg)\n",
        "    y_pred = model.predict(X_test_reg)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred))\n",
        "    r2 = r2_score(y_test_reg, y_pred)\n",
        "    regression_results[name] = {'RMSE': rmse, 'R2': r2}\n",
        "    print(f\"{name}: RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
        "\n",
        "# Summary DataFrame\n",
        "results_df = pd.DataFrame(regression_results).T\n",
        "print(\"\\n Regression Model Performance Summary:\")\n",
        "print(results_df)\n",
        "\n",
        "#  Bar plot of R² scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=results_df.reset_index(), x='index', y='R2', palette='coolwarm')\n",
        "plt.axhline(0, linestyle='--', color='gray')\n",
        "plt.title('R² Scores of Regression Models')\n",
        "plt.ylabel('R² Score')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gWSK5d5j4UMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# Define a pipeline that expands features with polynomials, scales them, and then fits XGBoost\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('xgb', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))\n",
        "])\n",
        "\n",
        "# Define a parameter grid for the pipeline (you can adjust these values)\n",
        "param_grid = {\n",
        "    'poly__degree': [2],  # Using degree 2 polynomial features (you can try degree 3 as well)\n",
        "    'xgb__n_estimators': [100, 150, 200],\n",
        "    'xgb__max_depth': [3, 5, 7],\n",
        "    'xgb__learning_rate': [0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "# Set up grid search with 3-fold cross-validation optimizing for R²\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
        "grid_search.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Display the best parameters\n",
        "print(\"Best parameters from GridSearchCV:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_reg)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred))\n",
        "r2 = r2_score(y_test_reg, y_pred)\n",
        "\n",
        "print(f\"XGBoost with Polynomial Features: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Tim234-Ltg9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 10. Visualize Regression Model Performance\n",
        "# ================================\n",
        "# Bar plot for RMSE comparison\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=results_df.index, y=results_df['RMSE'], palette='Blues_d')\n",
        "plt.title('Regression Models - RMSE Comparison')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Bar plot for R² comparison\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=results_df.index, y=results_df['R2'], palette='Greens_d')\n",
        "plt.title('Regression Models - R² Comparison')\n",
        "plt.ylabel('R² Score')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UHzv1gyZrJVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Fx2Wm9VyTId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 12.Time Series Forecasting with Prophet\n",
        "\n",
        "# Prepare data for forecasting the continuous target: volume_to_temp_ratio\n",
        "# Ensure 'volume_to_temp_ratio' column exists. If not, recalculate it.\n",
        "if 'volume_to_temp_ratio' not in merged_df.columns:\n",
        "    # Assuming you have the necessary columns ('Volume_std', 'air_temp_c_std')\n",
        "    merged_df['volume_to_temp_ratio'] = (merged_df['Volume_std'] / merged_df['air_temp_c_std']) * 100\n",
        "\n",
        "prophet_df = merged_df[['date_str', 'volume_to_temp_ratio']].copy()\n",
        "prophet_df.columns = ['ds', 'y']\n",
        "prophet_df['ds'] = pd.to_datetime(prophet_df['ds'], errors='coerce')\n",
        "prophet_df = prophet_df.dropna(subset=['ds'])\n",
        "\n",
        "prophet_model = Prophet()\n",
        "prophet_model.fit(prophet_df)\n",
        "\n",
        "# Forecast the next 30 days\n",
        "future = prophet_model.make_future_dataframe(periods=30)\n",
        "forecast = prophet_model.predict(future)\n",
        "fig = prophet_model.plot(forecast)\n",
        "plt.title(\"Prophet Forecast - Volume-to-Temp Ratio\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jllwo69_rM1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Date' in df_dispense.columns\n",
        "'Average Flow Rate' in merged_df.columns  # after merging\n"
      ],
      "metadata": {
        "id": "vZQJrYT3z_SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 14. Client-Requested Fixes and New Feature Engineering\n",
        "# ================================\n",
        "\n",
        "import pytz\n",
        "\n",
        "# Define Mountain timezone\n",
        "mountain = pytz.timezone('MST')\n",
        "\n",
        "# Convert 'Date' to datetime, localize to Mountain Time, then convert to UTC\n",
        "df_dispense['Date'] = pd.to_datetime(df_dispense['Date'], errors='coerce')\n",
        "df_dispense['Date_UTC'] = df_dispense['Date'].dt.tz_localize(mountain, ambiguous='NaT', nonexistent='NaT').dt.tz_convert('UTC')\n",
        "\n",
        "# Create date_str from UTC for merging\n",
        "df_dispense['date_str'] = df_dispense['Date_UTC'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Also fix the weather dataset\n",
        "df_weather['utc_date'] = pd.to_datetime(df_weather['utc_date'], errors='coerce')\n",
        "df_weather['date_str'] = df_weather['utc_date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Re-merge using date_str based on UTC\n",
        "merged_df = pd.merge(df_dispense, df_weather, on='date_str', how='inner')\n",
        "print(\"✅ Merged shape with UTC fixed:\", merged_df.shape)\n"
      ],
      "metadata": {
        "id": "a4f7uie60SJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numeric in case any columns have wrong types\n",
        "merged_df['Average Flow Rate'] = pd.to_numeric(merged_df['Average Flow Rate'], errors='coerce')\n",
        "merged_df['Flow Duration'] = pd.to_numeric(merged_df['Flow Duration'], errors='coerce')\n",
        "merged_df['Duration in Seconds'] = pd.to_numeric(merged_df['Duration in Seconds'], errors='coerce')\n",
        "\n",
        "# Create Downtime = Duration - Flow Duration\n",
        "merged_df['Downtime'] = merged_df['Duration in Seconds'] - merged_df['Flow Duration']\n",
        "\n",
        "# Preview result\n",
        "print(\"✅ Sample of Downtime values:\\n\", merged_df[['Duration in Seconds', 'Flow Duration', 'Downtime']].head())\n"
      ],
      "metadata": {
        "id": "gz1oAWUh0UqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Flow Duration'].isna().sum(), merged_df['Flow Duration'].shape[0]\n"
      ],
      "metadata": {
        "id": "Uy0wAlUV1OFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Product and Meter ID to find average flow rate\n",
        "grouped_flow = merged_df.groupby(['Product', 'Meter ID'])['Average Flow Rate'].mean().reset_index(name='avg_flow_by_product_meter')\n",
        "\n",
        "# Merge this KPI back into the main DataFrame\n",
        "merged_df = pd.merge(merged_df, grouped_flow, on=['Product', 'Meter ID'], how='left')\n",
        "\n",
        "# Preview result\n",
        "print(\"Added avg_flow_by_product_meter:\\n\", merged_df[['Product', 'Meter ID', 'avg_flow_by_product_meter']].head())\n"
      ],
      "metadata": {
        "id": "Pbi7t1-w0YVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a histogram of avg_flow_by_product_meter\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(merged_df['avg_flow_by_product_meter'], bins=30, kde=True)\n",
        "plt.title('Distribution of Average Flow Rate by Product and Meter')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bi58ggC0yws6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classification logic\n",
        "def classify_flow(value):\n",
        "    if pd.isna(value):\n",
        "        return 'Unknown'\n",
        "    elif value < 20:\n",
        "        return 'LOW'\n",
        "    elif value < 50:\n",
        "        return 'MED'\n",
        "    else:\n",
        "        return 'HIGH'\n",
        "\n",
        "# Apply flow category label\n",
        "merged_df['Flow_Category'] = merged_df['avg_flow_by_product_meter'].apply(classify_flow)\n",
        "\n",
        "# Show distribution\n",
        "print(\"✅ Flow Category Distribution:\\n\", merged_df['Flow_Category'].value_counts())\n"
      ],
      "metadata": {
        "id": "Y0tafNpY0v3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a count plot of Flow_Category\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=merged_df, x='Flow_Category')\n",
        "plt.title('Distribution of Flow Categories')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kNU3bL4i0iD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#performance matrix\n",
        "impo"
      ],
      "metadata": {
        "id": "BEWF-abz3I_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 15. Classification Model: Predicting Flow Category (MED vs HIGH)\n",
        "# ================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Use only clean and relevant numeric features\n",
        "features = ['air_temp_c', 'humidity_percent', 'precipitation_mm', 'Average Flow Rate', 'Duration in Seconds']\n",
        "\n",
        "# Drop missing values\n",
        "clf_data = merged_df[features + ['Flow_Category']].dropna()\n",
        "\n",
        "# Encode MED = 0, HIGH = 1\n",
        "clf_data['Flow_Category_Encoded'] = clf_data['Flow_Category'].map({'MED': 0, 'HIGH': 1})\n",
        "\n",
        "# Define inputs and target\n",
        "X = clf_data[features]\n",
        "y = clf_data['Flow_Category_Encoded']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a lightweight Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\n✅ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\n✅ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "M9WVidcs2uu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}